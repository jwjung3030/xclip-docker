version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    # container_name: ollama  # 가능하면 제거(내부 DNS 깔끔)
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    profiles: ["ollama"]
    healthcheck:
      # 서버 준비가 느릴 수 있으니 여유 있게
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    networks:
      - ollama-net

  xclip-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        DEVICE: gpu
        BASE_IMAGE: nvidia/cuda:12.1.0-base-ubuntu20.04
    image: xclip-gpu
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=http://ollama:11434  # ⬅️ 여기서 localhost 쓰면 안 됨
    volumes:
      - ./:/app
    command: ["python3", "main.py"]
    depends_on:
      ollama:
        condition: service_healthy
    profiles: ["xclip-gpu"]
    networks:
      - ollama-net

  xclip-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        DEVICE: cpu
        BASE_IMAGE: python:3.8-slim
    image: xclip-cpu
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./:/app
    command: ["python3", "main.py"]
    depends_on:
      ollama:
        condition: service_healthy
    profiles: ["xclip-cpu"]
    networks:
      - ollama-net

networks:
  ollama-net:
    driver: bridge

